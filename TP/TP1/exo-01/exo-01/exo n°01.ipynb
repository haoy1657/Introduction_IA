{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q* Learning \n",
    "<br> \n",
    "In this notebook, we'll implement an agent <b>that plays FrozenLake.</b>\n",
    "<img src=\"frozenlake.png\" alt=\"Frozen Lake\"/>\n",
    "\n",
    "The goal of this game is <b>to go from the starting state (S) to the goal state (G)</b> by walking only on frozen tiles (F) and avoid holes (H).However, the ice is slippery, <b>so you won't always move in the direction you intend (stochastic environment)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import the dependencies\n",
    "We use 3 libraries:\n",
    "- `Numpy` for our Qtable\n",
    "- `OpenAI Gym` for our FrozenLake Environment\n",
    "- `Random` to generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the environment\n",
    "- Here we'll create the FrozenLake environment. \n",
    "- OpenAI Gym is a library <b> composed of many environments that we can use to train our agents.</b>\n",
    "- In our case we choose to use Frozen Lake.\n",
    "- Note that S is the subject, F is frozen, H is the hole, and G the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the Q-table and initialize it\n",
    "- Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size\n",
    "- OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size :  16\n",
      "action size : 4\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "print(\"state size : \", state_size)\n",
    "print(\"action size :\" , action_size)\n",
    "\n",
    "# 4 actions : left , down ,right ,up\n",
    "# 16 states : every state is different from the other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Complete the code to create the qtable from the action_size and state_size\n",
    "qtable = np.zeros((state_size,action_size)) # 16 rows (states) , 4 Columns (cause of the 4 actions avalaible)\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 states : F , H , G , Initial S** \\\n",
    "**4 actions** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the hyperparameters\n",
    "- Here, we'll specify the hyperparameters\n",
    "- Comment about the impact of each parameter in the learning process. You can run again the notebook for different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 15000  # Total of simulations  \n",
    "learning_rate = 0.8           \n",
    "max_steps = 99  # Numbers of the agent's steps allowed in the envirionnement               \n",
    "gamma = 0.95                  \n",
    "\n",
    "epsilon = 0.5    # exploration or explotation          \n",
    "max_epsilon = 1.0             \n",
    "min_epsilon = 0.01            \n",
    "decay_rate = 0.005            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAll the following parameters are needed to implement the q learning algorithm \\n\\nTotal episodes ==> the amount of simulations in the training process for our agent in the Frozen lake environnement\\n\\nmax_steps ==> Numbers of the agent's steps allowed in the envirionnement    \\n\\nLearning rate ==> number between 0 and 1 . Can be thought as how quickly the agent abandons the previous q value for the new one \\n(for a given state/action pair)\\n\\ngamma ==> discount rate . Rate for which we discount futur rewards and will determine the present value of futur rewards\\n\\nepsilon ==> Also called the exploration rate  , probability about making exploration or exploitation. Is sets to '1' so the agent will start exploring\\nthe environnement. As the agent learn more about the environnement in each new episode , epsilon will decay by some rate so that exploration \\nbecome less and less probable \\n\\nmax_epsilon and min_epsilon :  bounds -> how large and how small the exploration rate (epsilon) can be \\n\\nDecay rate ==> Rate at whitch epslion will decay\\n \\n \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your comments in this cell:\n",
    "'''\n",
    "All the following parameters are needed to implement the q learning algorithm \n",
    "\n",
    "Total episodes ==> the amount of simulations in the training process for our agent in the Frozen lake environnement\n",
    "\n",
    "max_steps ==> Numbers of the agent's steps allowed in the envirionnement    \n",
    "\n",
    "Learning rate ==> number between 0 and 1 . Can be thought as how quickly the agent abandons the previous q value for the new one \n",
    "(for a given state/action pair)\n",
    "\n",
    "gamma ==> discount rate . Rate for which we discount futur rewards and will determine the present value of futur rewards\n",
    "\n",
    "epsilon ==> Also called the exploration rate  , probability about making exploration or exploitation. Is sets to '1' so the agent will start exploring\n",
    "the environnement. As the agent learn more about the environnement in each new episode , epsilon will decay by some rate so that exploration \n",
    "become less and less probable \n",
    "\n",
    "max_epsilon and min_epsilon :  bounds -> how large and how small the exploration rate (epsilon) can be \n",
    "\n",
    "Decay rate ==> Rate at whitch epslion will decay\n",
    " \n",
    " '''\n",
    "\n",
    "# Decay ==> dÃ©clin / decroit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: The Q learning algorithm\n",
    "- Now we implement the Q learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.4660666666666667\n",
      "[[4.44396227e-01 7.35355072e-03 7.40322777e-03 1.38931236e-02]\n",
      " [1.01612680e-04 1.87391530e-04 2.59414318e-05 3.86557465e-02]\n",
      " [6.80060804e-04 1.92746543e-03 1.09189374e-03 7.02107342e-03]\n",
      " [7.01953981e-04 2.20036014e-04 7.67445361e-04 7.22113981e-03]\n",
      " [2.83551193e-01 6.02100455e-03 2.11194642e-03 6.38495863e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.06255085e-05 7.41143369e-02 6.46964165e-06 6.69511892e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.75428970e-03 2.28351249e-03 3.99284027e-03 2.43091571e-01]\n",
      " [1.20084524e-02 2.64492377e-03 1.34817902e-01 2.46398996e-03]\n",
      " [6.76196167e-01 2.77473226e-04 1.40227280e-04 1.23716631e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.64343029e-02 3.08653262e-02 7.15919248e-01 4.91121728e-02]\n",
      " [1.38042707e-01 9.59749405e-01 1.25157041e-01 1.18111599e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1 --> Q-values are already initialized.\n",
    "\n",
    "# List of rewards we'll get from each episodes\n",
    "rewards = []\n",
    "\n",
    "# Step 2 --> For life or until learning is stopped ...\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    \n",
    "    done = False # the done variable just keeps track of whether or not the episode is finished \n",
    "    # We initialize it to false when we first start the episode \n",
    "    \n",
    "    total_rewards = 0 # Each epiosde starts with no rewards \n",
    "\n",
    "    \n",
    "    for step in range(max_steps): \n",
    "        # Step 3 --> Choose an action (a) in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        \n",
    "        \n",
    "        exp_exp_tradeoff = random.uniform(0,1) # Random - between 0 and 1 . Will be used to determine whether or not\n",
    "        # the agent will explore or exloit the environnement in this time step \n",
    "\n",
    "        # eps is initialy sets to 1 \n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for the current state)\n",
    "        if exp_exp_tradeoff > epsilon: \n",
    "            action = np.argmax(qtable[state]) \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample() # exploring the environnement and sample an action randomly \n",
    "            \n",
    "            # Random actions are set with the following command  : env.action_space.sample()\n",
    "\n",
    "        \n",
    "        # Step 4 --> Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action) # use the method step from env. Check gym library documentation.\n",
    "\n",
    "        # step returns a tuple containing  : \n",
    "        # - The new state \n",
    "        # - The reward for the action we took \n",
    "        # - whether or not our action ended our episode \n",
    "        # - Some diagnostic info regarding our environnement\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Step 5 --> Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        \n",
    "        \n",
    "        #Bellman's equation : \n",
    "        \n",
    "        qtable[state, action] = qtable[state, action] + learning_rate *(reward + gamma*np.max(qtable[new_state]) - qtable[state, action])\n",
    "        \n",
    "        #the new q value for this \"state/action\" pair, is a weighted sum of out old values and learn values \n",
    "        # \n",
    "        # Learn values :  learning_rate *(reward + gamma*np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        \n",
    "        total_rewards += reward\n",
    "\n",
    "\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon \n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "    # Why do we need to reduce the epsilon? Comment below:\n",
    "    '''\n",
    "    Write your answer here:\n",
    "     the more epsilon is small , the more the condition exp_exp_tradeoff > epsilon will be verified , which leads to more exploitation from \n",
    "     the agent and less exploration , of our environnement.\n",
    "\n",
    "     if for instance we decide to define epsilon as equal to 0.5 ,  we'll observe in Step 5 that the agent will  ends up to the goal five times in\n",
    "     a row.\n",
    "    \n",
    "    '''\n",
    "   \n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)\n",
    "\n",
    "#  Epsilon will decrease as the agent evolve throught the environnement during the severals epiosdes\n",
    "# As epsilon decrease , we will have more chance to avec tradeoff > epsilon , so more exploitation as we learn of the environnement\n",
    "# Principe of the \"apprentissage par renforcement\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Use our Q-table to play FrozenLake! \n",
    "- After 10 000 episodes, our Q-table can be used as a \"cheatsheet\" to play FrozenLake\"\n",
    "- By running this cell you can see our agent playing FrozenLake.\n",
    "- Evaluate how the reward evolves in each step and comment about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Number of steps 21\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Number of steps 14\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Number of steps 15\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 18\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 19\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):  \n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state]) #qtable defined in the previous question after 10.000 episodes \n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x227eb97e430>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write here your code to evaluate the evolution of the reward in each step.\n",
    "\n",
    "# type(rewards) --> list \n",
    "# len(rewards) --> 6856 elements \n",
    "\n",
    "# Average rewards each 1000 episodes \n",
    "\n",
    "Rewards_per_episode  = np.split(np.array(rewards) , total_episodes/1000)\n",
    "count = 1000\n",
    "V = np.zeros(15)\n",
    "i = 0 \n",
    "for k in Rewards_per_episode : \n",
    "    V[i] = str(sum(k/1000))\n",
    "    count+= 1000\n",
    "    i+= 1 \n",
    "\n",
    "\n",
    "Epi = np.linspace(1000,15000,15)  \n",
    "\n",
    "plt.plot(Epi,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your comments in this cell\n",
    "'''\n",
    "The average reward will oscillate between a certain domain after \n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6aa1a8b2ed28e89771df5352948886da4de96dc4e60e47b144f18524ed7d54c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
